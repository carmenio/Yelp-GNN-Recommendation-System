{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source: https://business.yelp.com/data/resources/open-dataset/\n",
    "\n",
    "This code filters the data contained in the JSON files to only include items that relate to specific businesses in specific cities.\n",
    "\n",
    "It is based on a dumbed down version of chris' Data Loaders.\n",
    "\n",
    "I find that dictionary-based data structures are fine enough for our purposes without over-complicating things. Must bear in mind that the ultimate goal is to convert these data into pytorch-geometric data structures to run GNN models on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick summary stats to work out what filters to apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150346it [00:00, 207052.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of businesses: 150346\n",
      "=== Top 5 by count... ===\n",
      "Cities: {'Philadelphia': 14569, 'Tucson': 9250, 'Tampa': 9050, 'Indianapolis': 7540, 'Nashville': 6971}\n",
      "States: {'PA': 34039, 'FL': 26330, 'TN': 12056, 'IN': 11247, 'MO': 10913}\n",
      "Categories: {'Restaurants': 52268, 'Food': 27781, 'Shopping': 24395, 'Home Services': 14356, 'Beauty & Spas': 14292}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import heapq\n",
    "\n",
    "city_count = Counter()\n",
    "state_count = Counter()\n",
    "category_count = Counter()\n",
    "total_count = 0\n",
    "\n",
    "with open(\"../../Data/yelp_academic_dataset_business.json\", \"r\") as file:\n",
    "    for line in tqdm(islice(file, None)):\n",
    "        total_count += 1\n",
    "        data = json.loads(line)\n",
    "\n",
    "        city_count[data['city']] += 1\n",
    "        state_count[data['state']] += 1\n",
    "\n",
    "        if isinstance(data.get('categories'), str):\n",
    "            for category in data['categories'].split(', '):\n",
    "                category_count[category] += 1\n",
    "\n",
    "print(\"Total number of businesses:\", total_count)\n",
    "print(\"=== Top 5 by count... ===\")\n",
    "print(\"Cities:\", dict(heapq.nlargest(5, city_count.items(), key=lambda item: item[1])))\n",
    "print(\"States:\", dict(heapq.nlargest(5, state_count.items(), key=lambda item: item[1])))\n",
    "print(\"Categories:\", dict(heapq.nlargest(5, category_count.items(), key=lambda item: item[1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Filter Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150346it [00:00, 263168.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14576 unique businesses from Philadelphia loaded from yelp_academic_dataset_business.json\n"
     ]
    }
   ],
   "source": [
    "# Bare bones data structure, a list of dictionaries representing businesses in a selected city \n",
    "# and a list of unique business_ids to use to look up with customers to keep\n",
    "business_limit = None\n",
    "city='Philadelphia'\n",
    "business_list = []\n",
    "business_id_set=set()\n",
    "with open(\"../../Data/yelp_academic_dataset_business.json\", \"r\") as file:\n",
    "    for line in tqdm(islice(file, business_limit)):\n",
    "        business=json.loads(line)\n",
    "        \n",
    "        # Filter to selected city AND exclude duplicates\n",
    "        if business['city'].upper()==city.upper() and business['business_id'] not in business_id_set:\n",
    "            business_list.append(business)\n",
    "            business_id_set.add(business['business_id'])\n",
    "\n",
    "print(f'{len(business_id_set)} unique businesses from {city} loaded from yelp_academic_dataset_business.json')\n",
    "\n",
    "# Load filtered lists to pandas dataframes for easier variable processing and file saving\n",
    "business_df=pd.DataFrame(business_list)\n",
    "business_df.to_parquet('../../Data/business.parquet', engine='pyarrow', compression='snappy')\n",
    "del business_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and filter reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4920154it [00:16, 293069.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 4920153 failed: Expecting ',' delimiter: line 1 column 135 (char 134)\n",
      "686422 unique reviews of businesses from Philadelphia posted from 227407 unique users have been loaded and saved to review.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "review_user_id_set = set()\n",
    "review_id_set = set()\n",
    "buffer = []\n",
    "\n",
    "parquet_writer = None\n",
    "batch_size = 10000\n",
    "error_count = 0\n",
    "max_errors_to_show = 5\n",
    "\n",
    "with open(\"../../Data/yelp_academic_dataset_review.json\", \"r\") as file:\n",
    "    for i, line in enumerate(tqdm(islice(file, None))):\n",
    "        try:\n",
    "            review = json.loads(line)\n",
    "            # Safeguard access to required keys\n",
    "            if all(k in review for k in ['business_id', 'review_id', 'user_id']):\n",
    "                if review['business_id'] in business_id_set and review['review_id'] not in review_id_set:\n",
    "                    buffer.append(review)\n",
    "                    review_user_id_set.add(review['user_id'])\n",
    "                    review_id_set.add(review['review_id'])\n",
    "\n",
    "                    if len(buffer) >= batch_size:\n",
    "                        df = pd.DataFrame(buffer)\n",
    "                        table = pa.Table.from_pandas(df)\n",
    "                        if parquet_writer is None:\n",
    "                            parquet_writer = pq.ParquetWriter('../../Data/review.parquet', table.schema, compression='snappy')\n",
    "                        parquet_writer.write_table(table)\n",
    "                        buffer.clear()\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            if error_count <= max_errors_to_show:\n",
    "                print(f'Line {i} failed: {e}')\n",
    "            elif error_count == max_errors_to_show + 1:\n",
    "                print(\"More errors encountered, suppressing further error messages...\")\n",
    "\n",
    "# Write remaining records\n",
    "if buffer:\n",
    "    df = pd.DataFrame(buffer)\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    if parquet_writer is None:\n",
    "        parquet_writer = pq.ParquetWriter('../../Data/review.parquet', table.schema, compression='snappy')\n",
    "    parquet_writer.write_table(table)\n",
    "\n",
    "if parquet_writer:\n",
    "    parquet_writer.close()\n",
    "\n",
    "print(f'{len(review_id_set)} unique reviews of businesses from {city} posted from {len(review_user_id_set)} unique users have been loaded and saved to review.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and filter users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1987897it [00:09, 204091.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227407 unique users that have posted at least one review for a business in Philadelphia have been loaded and saved to user.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_id_set = set()\n",
    "buffer = []\n",
    "\n",
    "parquet_writer = None\n",
    "batch_size = 10000\n",
    "error_count = 0\n",
    "max_errors_to_show = 5\n",
    "\n",
    "with open(\"../../Data/yelp_academic_dataset_user.json\", \"r\") as file:\n",
    "    for i, line in enumerate(tqdm(islice(file, None))):\n",
    "        try:\n",
    "            user = json.loads(line)\n",
    "            if 'user_id' in user and user['user_id'] in review_user_id_set and user['user_id'] not in user_id_set:\n",
    "                buffer.append(user)\n",
    "                user_id_set.add(user['user_id'])\n",
    "\n",
    "                if len(buffer) >= batch_size:\n",
    "                    df = pd.DataFrame(buffer)\n",
    "                    table = pa.Table.from_pandas(df)\n",
    "                    if parquet_writer is None:\n",
    "                        parquet_writer = pq.ParquetWriter('../../Data/user.parquet', table.schema, compression='snappy')\n",
    "                    parquet_writer.write_table(table)\n",
    "                    buffer.clear()\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            if error_count <= max_errors_to_show:\n",
    "                print(f'Line {i} failed: {e}')\n",
    "            elif error_count == max_errors_to_show + 1:\n",
    "                print(\"More errors encountered, suppressing further error messages...\")\n",
    "\n",
    "# Write any remaining users\n",
    "if buffer:\n",
    "    df = pd.DataFrame(buffer)\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    if parquet_writer is None:\n",
    "        parquet_writer = pq.ParquetWriter('../../Data/user.parquet', table.schema, compression='snappy')\n",
    "    parquet_writer.write_table(table)\n",
    "\n",
    "if parquet_writer:\n",
    "    parquet_writer.close()\n",
    "\n",
    "print(f'{len(user_id_set)} unique users that have posted at least one review for a business in {city} have been loaded and saved to user.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del batch_size, buffer, business, business_id_set, business_limit, business_list, category, category_count, city, city_count, data\n",
    "del df, error_count, file, i, line, max_errors_to_show, parquet_writer, review, review_id_set, review_user_id_set\n",
    "del state_count, table, total_count, user, user_id_set\n",
    "# Clean up any remaining variables\n",
    "#del globals()[name]  # Uncomment to delete all variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify most recent timestamp for normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Record Age Summary ===\n",
      "max_ts:         2022-01-19 19:46:34\n",
      "min_ts:         2004-10-12 09:36:53\n",
      "avg_age(days):  2402.0728801559153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Reload from .parquet files\n",
    "user_df = pd.read_parquet('../../Data/user.parquet', engine='pyarrow')\n",
    "review_df = pd.read_parquet('../../Data/review.parquet', engine='pyarrow')\n",
    "\n",
    "# Convert TS to Datetime\n",
    "user_df['yelping_since']=pd.to_datetime(user_df['yelping_since'])\n",
    "review_df['date']=pd.to_datetime(review_df['date'])\n",
    "\n",
    "# Get most recent timestamp across all records\n",
    "ts_list=user_df['yelping_since'].to_list()+review_df['date'].to_list()\n",
    "\n",
    "max_ts=max(ts_list)\n",
    "min_ts=min(ts_list)\n",
    "age_ts_list=[max_ts-ts for ts in ts_list]\n",
    "age_days_list=[ts.days for ts in age_ts_list]\n",
    "avg_age=sum(age_days_list)/len(age_days_list)\n",
    "\n",
    "print(f\"\"\"\n",
    "=== Record Age Summary ===\n",
    "max_ts:         {max_ts}\n",
    "min_ts:         {min_ts}\n",
    "avg_age(days):  {avg_age}\n",
    "\"\"\")\n",
    "\n",
    "# Open the file in write mode ('w')\n",
    "with open('../../Data/max_ts.txt', 'w') as file:\n",
    "    # Write the timestamp to the file as a string\n",
    "    file.write(str(max_ts))\n",
    "\n",
    "del user_df, review_df, max_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5473"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_ts_list[0].days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: User Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#=== Reload from .parquet files ===\n",
    "user_df = pd.read_parquet('../../Data/user.parquet', engine='pyarrow')\n",
    "\n",
    "#=== Reload max_ts from text file ===\n",
    "with open('../../Data/max_ts.txt', \"r\") as file:\n",
    "    max_ts=pd.to_datetime(file.read().strip())\n",
    "\n",
    "#=== Calculate record age ===\n",
    "user_df['yelping_since']=pd.to_datetime(user_df['yelping_since'])\n",
    "user_df['record_age']=max_ts-user_df['yelping_since']\n",
    "user_df['record_age']=user_df['record_age'].dt.days\n",
    "\n",
    "#=== Call reset_index() to provide index mapping between [user_id] and a machine readable integer ===\n",
    "user_df.reset_index(inplace=True)\n",
    "user_df.rename(columns={'index':'user_id_index'},inplace=True)\n",
    "\n",
    "#=== Convert friends to list of indices and filter to ensure only friends within the user_id_set are present ===\n",
    "user_id_set={user_id for user_id in user_df['user_id'].to_list()} # Find unique set of users\n",
    "user_id_map=pd.Series(user_df['user_id_index'].values, index=user_df['user_id']).to_dict() # Create mapping dict between user ID and its' index\n",
    "user_df['friends']=user_df['friends'].apply(lambda friend_list: [user_id_map.get(friend) for friend in friend_list.split(', ') if friend in user_id_set])\n",
    "user_df['friend_count']=user_df['friends'].apply(lambda friend_list: len(friend_list))\n",
    "\n",
    "#=== Derive elite variables ===\n",
    "user_df['elite']=user_df['elite'].apply(lambda elite_years: [pd.to_datetime(datetime(year=int(year), month=1, day=1), errors='coerce')\n",
    "                                                             for year in elite_years.split(\",\")\n",
    "                                                             if year.isdigit()\n",
    "                                                             ])\n",
    "user_df['elite_count']=user_df['elite'].apply(lambda elite_years: len(elite_years))\n",
    "user_df['latest_elite']=user_df['elite'].apply(lambda elite_years: max(elite_years) if len(elite_years)>0 else pd.NaT)\n",
    "user_df['elite_age']=user_df['latest_elite'].apply(lambda latest_elite: max_ts-latest_elite)\n",
    "user_df['elite_age']=user_df['elite_age'].dt.days\n",
    "\n",
    "#=== Drop columns that are no longer needed ===\n",
    "user_df.drop(columns=['latest_elite','yelping_since'], inplace=True)\n",
    "\n",
    "#=== Apply z-score scaling to numeric variables besides user_id_index ===\n",
    "cols_to_scale=user_df.select_dtypes(exclude=['object']).columns.tolist()\n",
    "cols_to_scale.remove('user_id_index')\n",
    "scaler = StandardScaler()\n",
    "user_df[cols_to_scale]=scaler.fit_transform(user_df[cols_to_scale])\n",
    "\n",
    "#=== Write to .parquet file ===\n",
    "user_df.to_parquet('../../Data/preprocessed_user.parquet', engine='pyarrow', compression='snappy')\n",
    "\n",
    "# === Final Post-processed version user_df ===\n",
    "user_df.head()\n",
    "\n",
    "#=== remove unneeded variables ===\n",
    "del cols_to_scale, scaler, user_id_set, user_id_map, user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing: Business features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test business_df['categories'] for suitability of 'all-MiniLM-L6-v2'  \n",
      "=== for embedding calculation\n",
      "Max num Characters: 503\n",
      "Max num Tokens: 36\n",
      "Num unique categories: 1027\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#=== Reload from .parquet files ===\n",
    "business_df = pd.read_parquet('../../Data/business.parquet', engine='pyarrow')\n",
    "\n",
    "#=== Process categories ===\n",
    "# Two competing methods applied here\n",
    "# 1) Categories are treated as graph nodes, and relationships are recorded as edges\n",
    "# 2) Categories are text embeddings, and the full list is transformed via pretrained LLM\n",
    "\n",
    "#=== 1) Graph nodes + egdes\n",
    "def split_text(string_var):\n",
    "    \"\"\"\n",
    "    Helper function to allow .split(', ') to not fail if type is not Str.\n",
    "    \"\"\"\n",
    "    if type(string_var)==str:\n",
    "        return string_var.split(', ')\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "business_df['category_list']=business_df['categories'].apply(lambda x: split_text(x))\n",
    "\n",
    "categories={category\n",
    "            for category_list in business_df['category_list'].to_list()\n",
    "            for category in category_list}\n",
    "category_index={category:idx for idx,category in enumerate(categories)}\n",
    "\n",
    "# Saving business_category_index for later use\n",
    "with open('../../Data/business_category_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(category_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "business_df['category_index']=business_df['category_list'].apply(lambda category_list: [category_index.get(category) for category in category_list])\n",
    "\n",
    "#=== 2) Category Embeddings\n",
    "print(\"=== Test business_df['categories'] for suitability of 'all-MiniLM-L6-v2' \"\n",
    ",\"\\n=== for embedding calculation\")\n",
    "print('Max num Characters:',max([len(category_list) \n",
    "     for category_list in business_df['categories'].to_list()\n",
    "     if type(category_list)==str]))\n",
    "print('Max num Tokens:',max([len(category_list.split(', ')) \n",
    "     for category_list in business_df['categories'].to_list()\n",
    "     if type(category_list)==str]))\n",
    "\n",
    "print('Num unique categories:',len(categories))\n",
    "\n",
    "del categories, category_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** I'm using **all-MiniLM-L6-v2** like a hammer and all business_df columns as nails.  \n",
    "Some other treatments we can do are:\n",
    "* Create a 1-hot-encoded vector embedding for the 1027 categories - **EASY** - Vectors will be a sparse representation and would likely not be very predictive\n",
    "* Create a dense vector representation for categories - **HARD** - Will need to create a separate predictive model (possibly ALBERT/BERT/ETC) to come up with this, value may or may not be better than just using **all-MiniLM-L6-v2**\n",
    "* Add a Geo-encoding vector model for latitude & longitude - **MODERATE** - I haven't found one yet, probably straightforward if found\n",
    "* Add a Geo-location hierarchy to the heterogenous graph network - **HARD** - This has been done in some papers. Interesting direction to take AFTER building POC\n",
    "* Add another one-hot-encoded attribute vector from 'attributes' - **EASY** - This may be better off as a set of post model update features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherarmenio/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6436d9e6821943ea9d80ad48aeac1761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5ad7de329149a2a7ae18185b83c65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbbc71e04c54e688e73aef3e9268d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text pre-processing\n",
    "def preprocess_text(string_var):\n",
    "    if type(string_var)==str:\n",
    "        string_var=string_var.lower()\n",
    "        string_var=re.sub(r'[^a-zA-Z0-9\\s]', '', string_var)\n",
    "        return string_var\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "business_df['categories-cleaned']=business_df['categories'].apply(lambda x: preprocess_text(x))\n",
    "business_df['name-cleaned']=business_df['name'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "# Must append address + city if including multiple cities in this dataset\n",
    "business_df['address-cleaned']=business_df['address'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "with torch.no_grad():\n",
    "    category_embeddings = model.encode(business_df['categories-cleaned'].to_list(), convert_to_tensor=True, show_progress_bar=True)\n",
    "    name_embeddings     = model.encode(business_df['name-cleaned'].to_list(), convert_to_tensor=True, show_progress_bar=True)\n",
    "    address_embeddings  = model.encode(business_df['address-cleaned'].to_list(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "    category_embeddings = category_embeddings.cpu()\n",
    "    name_embeddings     = name_embeddings.cpu()\n",
    "    address_embeddings  = address_embeddings.cpu()\n",
    "\n",
    "business_df['category_embeddings']=category_embeddings.tolist()\n",
    "business_df['name_embeddings']=name_embeddings.tolist()\n",
    "business_df['address_embeddings']=address_embeddings.tolist()\n",
    "\n",
    "del model, category_embeddings, name_embeddings, address_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalisation for review counts\n",
    "scaler = StandardScaler()\n",
    "business_df['review_count']=scaler.fit_transform(pd.DataFrame(business_df['review_count']))\n",
    "\n",
    "# Min-max scaling for average ratings\n",
    "business_df['stars']=(business_df['stars'] - 1.0)/4.0\n",
    "\n",
    "business_df.reset_index(inplace=True)\n",
    "business_df.rename(columns={'index':'business_id_index'},inplace=True)\n",
    "business_df.head()\n",
    "\n",
    "del scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df.to_parquet('../../Data/preprocessed_business.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del business_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing: Review features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test review_df['text'] for suitability of 'all-MiniLM-L6-v2'  \n",
      "=== for embedding calculation, must be 512 tokens or less...\n",
      "Max num Characters: 5000\n",
      "Max num Tokens: 3079\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#=== Reload from .parquet files ===\n",
    "review_df = pd.read_parquet('../../Data/review.parquet', engine='pyarrow')\n",
    "user_df = pd.read_parquet('../../Data/preprocessed_user.parquet', engine='pyarrow')[['user_id','user_id_index']]\n",
    "business_df = pd.read_parquet('../../Data/preprocessed_business.parquet', engine='pyarrow')[['business_id','business_id_index']]\n",
    "\n",
    "# Reload max_ts from text file\n",
    "with open('../../Data/max_ts.txt', \"r\") as file:\n",
    "    max_ts=pd.to_datetime(file.read().strip())\n",
    "\n",
    "#=== Calculate record age ===\n",
    "review_df['date']=pd.to_datetime(review_df['date'])\n",
    "review_df['record_age']=max_ts-review_df['date']\n",
    "review_df['record_age']=review_df['record_age'].dt.days\n",
    "\n",
    "#=== Add id index mappings from user and business hierarchies ===\n",
    "review_df = review_df.merge(user_df[['user_id','user_id_index']], on='user_id')\n",
    "review_df = review_df.merge(business_df[['business_id','business_id_index']], on='business_id')\n",
    "review_df.reset_index(inplace=True)\n",
    "review_df.rename(columns={'index':'review_id_index'},inplace=True)\n",
    "\n",
    "#=== Min-max scaling for average ratings ===\n",
    "review_df['stars']=(review_df['stars'] - 1.0)/4.0\n",
    "\n",
    "# !!! CANNOT PERFROM Z-SCORE NORMALISATION YET AS IT WOULD BIAS THE STAR REGRESSION PREDICTIONS !!!\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# Z-score normalisation for review counts\n",
    "# cols_to_scale=['useful','funny','cool']\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# business_df['review_count']=scaler.fit_transform(pd.DataFrame(business_df['review_count']))\n",
    "\n",
    "print(\"=== Test review_df['text'] for suitability of 'all-MiniLM-L6-v2' \"\n",
    ",\"\\n=== for embedding calculation, must be 512 tokens or less...\")\n",
    "print('Max num Characters:',max([len(review_text)\n",
    "     for review_text in review_df['text'].to_list()\n",
    "     if type(review_text)==str]))\n",
    "print('Max num Tokens:',max([len(review_text.split(' '))\n",
    "     for review_text in review_df['text'].to_list()\n",
    "     if type(review_text)==str]))\n",
    "\n",
    "#=== Apply SentenceTransformer BERT model to generate embeddings of ===\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import re\n",
    "# Text pre-processing\n",
    "# def preprocess_text(string_var):\n",
    "#     if type(string_var)==str:\n",
    "#         string_var=string_var.lower()\n",
    "#         string_var=re.sub(r'[^a-zA-Z0-9\\s]', '', string_var)\n",
    "#         return string_var\n",
    "#     else:\n",
    "#         return ''\n",
    "    \n",
    "# review_df['text-cleaned']=review_df['text'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     text_embeddings = model.encode(review_df['text-cleaned'].to_list(), convert_to_tensor=True, show_progress_bar=True)\n",
    "#     text_embeddings = text_embeddings.cpu()\n",
    "\n",
    "# review_df['text_embeddings']=text_embeddings.tolist()\n",
    "\n",
    "# del model, text_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the maximum review length cannot be encoded by **all-MiniLM-L6-v2**. That said, a truncated review is better than nothing. hence the following block. We can potentially fix this later either via a chunking methodology, a long context model, or topic modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id_index</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>record_age</th>\n",
       "      <th>user_id_index</th>\n",
       "      <th>business_id_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n",
       "      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n",
       "      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I've taken a lot of spin classes over the year...</td>\n",
       "      <td>2012-01-03 15:28:18</td>\n",
       "      <td>3669</td>\n",
       "      <td>14759</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>2015-01-04 00:01:03</td>\n",
       "      <td>2572</td>\n",
       "      <td>37700</td>\n",
       "      <td>622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>JrIxlS1TzJ-iCu79ul40cQ</td>\n",
       "      <td>eUta8W_HdHMXPzLBBZhL1A</td>\n",
       "      <td>04UD14gamNjLY0IDYVhHJg</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>I am a long term frequent customer of this est...</td>\n",
       "      <td>2015-09-23 23:10:31</td>\n",
       "      <td>2309</td>\n",
       "      <td>31304</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8JFGBuHMoiNDyfcxuWNtrA</td>\n",
       "      <td>smOvOajNG0lS4Pq7d8g4JQ</td>\n",
       "      <td>RZtGWDLCAtuipwaZ-UfjmQ</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good food--loved the gnocchi with marinara\\nth...</td>\n",
       "      <td>2009-10-14 19:57:14</td>\n",
       "      <td>4479</td>\n",
       "      <td>7721</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>oyaMhzBSwfGgemSGuZCdwQ</td>\n",
       "      <td>Dd1jQj7S-BFGqRbApFzCFw</td>\n",
       "      <td>YtSqYv1Q_pOltsVPSx54SA</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Tremendous service (Big shout out to Douglas) ...</td>\n",
       "      <td>2013-06-24 11:21:25</td>\n",
       "      <td>3131</td>\n",
       "      <td>29013</td>\n",
       "      <td>1130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_id_index               review_id                 user_id  \\\n",
       "0                0  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q   \n",
       "1                1  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ   \n",
       "2                2  JrIxlS1TzJ-iCu79ul40cQ  eUta8W_HdHMXPzLBBZhL1A   \n",
       "3                3  8JFGBuHMoiNDyfcxuWNtrA  smOvOajNG0lS4Pq7d8g4JQ   \n",
       "4                4  oyaMhzBSwfGgemSGuZCdwQ  Dd1jQj7S-BFGqRbApFzCFw   \n",
       "\n",
       "              business_id  stars  useful  funny  cool  \\\n",
       "0  7ATYjTIgM3jUlt4UM3IypQ   1.00       1      0     1   \n",
       "1  kxX2SOes4o-D3ZQBkiMRfA   1.00       1      0     1   \n",
       "2  04UD14gamNjLY0IDYVhHJg   0.00       1      2     1   \n",
       "3  RZtGWDLCAtuipwaZ-UfjmQ   0.75       0      0     0   \n",
       "4  YtSqYv1Q_pOltsVPSx54SA   1.00       0      0     0   \n",
       "\n",
       "                                                text                date  \\\n",
       "0  I've taken a lot of spin classes over the year... 2012-01-03 15:28:18   \n",
       "1  Wow!  Yummy, different,  delicious.   Our favo... 2015-01-04 00:01:03   \n",
       "2  I am a long term frequent customer of this est... 2015-09-23 23:10:31   \n",
       "3  Good food--loved the gnocchi with marinara\\nth... 2009-10-14 19:57:14   \n",
       "4  Tremendous service (Big shout out to Douglas) ... 2013-06-24 11:21:25   \n",
       "\n",
       "   record_age  user_id_index  business_id_index  \n",
       "0        3669          14759               1281  \n",
       "1        2572          37700                622  \n",
       "2        2309          31304                256  \n",
       "3        4479           7721                820  \n",
       "4        3131          29013               1130  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.to_parquet('../../Data/preprocessed_review.parquet', engine='pyarrow', compression='snappy')\n",
    "del review_df, business_df, user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "review_df = pd.read_parquet('../../Data/preprocessed_review.parquet', engine='pyarrow')\n",
    "user_df = pd.read_parquet('../../Data/preprocessed_user.parquet', engine='pyarrow')\n",
    "business_df = pd.read_parquet('../../Data/preprocessed_business.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Counts post-filtering ===\n",
      "business.json:14576\n",
      "user.json:    227407\n",
      "review.json:  686422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"=== Total Counts post-filtering ===\n",
    "business.json:{len(business_df)}\n",
    "user.json:    {len(user_df)}\n",
    "review.json:  {len(review_df)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345962"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(friends) for friends in user_df['friends'].to_list()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
